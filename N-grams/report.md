
For this task, we used the Brown corpus, which is a large collection of English texts from many different types of writing, like news, fiction, and academic articles. It is often used in language research and helps to study how words are used in real situations.

I focused on 2-grams, which means pairs of words that appear together in the text. For example, in the phrase "united states", the 2-gram is "united states". We call these sequences "n-grams" where "n" is the number of words in the sequence. So, 2-grams are pairs, 3-grams are triplets, and so on.

I chose 2-grams because they are easier to count and understand for basic next-word prediction. Using 3-grams or higher can give more precise context, but it needs more data and more processing power. For this practice, 2-grams were enough to see how word pairs predict the next word.

When I gave the input word "united", the most common next word was "states", showing the strong connection between these two words in English. Other next words were much less frequent.
