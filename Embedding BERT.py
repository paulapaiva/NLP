# -*- coding: utf-8 -*-
"""Assignment 3 - Paula Paiva.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKSzc1kc6RlqYgJFwJIYsKH1Lxvg20Zh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from transformers import BertTokenizer, BertModel
import torch

# load csv (write the right path)
df = pd.read_csv("/content/NikeProductDescriptions.csv")


print(df.columns)

subtitles_to_keep = ["Men's Shoes", "Men's T-Shirt", "Women's Shoes","Skate Shoes", "Older Kids' T-Shirt", "Shorts"]

df.loc[df['Subtitle'].str.contains("Shorts", case=False, na=False), 'Subtitle'] = "Shorts"

df = df[df['Subtitle'].isin(subtitles_to_keep)].reset_index(drop=True)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
model.eval()

def get_embedding(text):
    tokens = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**tokens)
    return outputs.last_hidden_state[0][0].numpy()

# embeddings
embeddings = []

for i in range(len(df)):
    desc = str(df.loc[i, 'Product Description'])
    emb = get_embedding(desc)
    embeddings.append(emb)

embeddings = np.array(embeddings)

pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)
df['pca1'] = reduced[:, 0]
df['pca2'] = reduced[:, 1]

colors = {
    "Men's Shoes": 'blue',
    "Men's T-Shirt": 'green',
    "Women's Shoes": 'red',
    "Skate Shoes": 'purple',
    "Older Kids' T-Shirt": 'orange',
    "Shorts": 'brown'
}

for subtitle in df['Subtitle'].unique():
    temp = df[df['Subtitle'] == subtitle]
    plt.scatter(temp['pca1'], temp['pca2'], label=subtitle, c=colors[subtitle], alpha=0.6)

plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.title("Products (BERT Embeddings + PCA)")
plt.legend()
plt.tight_layout()
plt.show()
