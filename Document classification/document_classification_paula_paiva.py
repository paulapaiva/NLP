# -*- coding: utf-8 -*-
"""Document Classification - Paula Paiva.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bzSC_uxzCz5o7i13J9Uw5WrhEj_VDM2M

Unsupervised Learning
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load dataset
df = pd.read_csv('/content/spam.csv')
df.columns

# Encode labels: ham = 0, spam = 1
label_encoder = LabelEncoder()
df['label_num'] = label_encoder.fit_transform(df['target'])

# TF-IDF vectorization
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(df['text'])

# Dimensionality reduction with PCA (optional)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_tfidf.toarray())

# KMeans clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(X_pca)

# Hierarchical clustering
agg = AgglomerativeClustering(n_clusters=2)
agg_labels = agg.fit_predict(X_pca)


# Plot clusters (KMeans)
plt.subplot(1, 2, 1)
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=kmeans_labels, palette='Set1')
plt.title('KMeans Clustering')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(title='Cluster')

# Plot true labels
plt.subplot(1, 2, 2)
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['target'], palette='Set2')
plt.title('Ground Truth Labels (Spam/Ham)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(title='target')

plt.tight_layout()
plt.show()

"""Supervised Learning"""

!pip install gensim scikit-learn xgboost

!pip install --upgrade --force-reinstall numpy==1.24.4 gensim==4.3.2 scikit-learn==1.3.2

import re
import nltk
from nltk.corpus import stopwords
nltk.download('punkt_tab')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def preprocess(text):
    text = re.sub(r'\W+', ' ', text.lower())
    tokens = nltk.word_tokenize(text)
    return [t for t in tokens if t not in stop_words]

df['tokens'] = df['text'].apply(preprocess)

!pip install --force-reinstall --no-cache-dir scipy==1.11.4

from gensim.models import Word2Vec

#  Word2Vec
w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)

# phrase to vector
import numpy as np

def get_average_w2v(tokens):
    vectors = [w2v_model.wv[w] for w in tokens if w in w2v_model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)

X_w2v = np.vstack(df['tokens'].apply(get_average_w2v))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

y = LabelEncoder().fit_transform(df['target'])  # spam = 1, ham = 0

X_train, X_test, y_train, y_test = train_test_split(X_w2v, y, test_size=0.2, random_state=42)

from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

models = {
    "Naive Bayes": GaussianNB(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    print(f"{name} accuracy: {acc:.4f}")